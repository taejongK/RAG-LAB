{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from operator import itemgetter\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pickle\n",
    "\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 저장된 임베딩 모델 불러오기\n",
    "embedding_model_path = \"/home/taejong_kim/workspace/rag-lab/database/embedding_model/hf_embedding_model01.pkl\"\n",
    "vectorstore_path = \"/home/taejong_kim/workspace/rag-lab/database/vector_store\"\n",
    "\n",
    "with open(embedding_model_path, \"rb\") as f:\n",
    "    embedding_model = pickle.load(f)\n",
    "\n",
    "# 2. 저장된 벡터스토어 불러오기\n",
    "vectorstore = FAISS.load_local(\n",
    "    vectorstore_path, embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "# 3. retriever 생성\n",
    "retrieval = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    answer: str = Field(..., description=\"The answer to the user's question\")\n",
    "    is_context_relevant: bool = Field(\n",
    "        False, description=\"Returns 'True' if the response is relevant to the context, otherwise 'False'.\"\n",
    "    )\n",
    "    \n",
    "json_parser = JsonOutputParser(pydantic_object=Answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\"You are a support agent. \n",
    "Please respond in the same language as the user's input. Detect the language they are using and reply naturally in that language while maintaining clarity and accuracy.\n",
    "\n",
    "If you don't know the answer, just say that you don't know. \n",
    "\n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "\n",
    "#Previous Chat History:\n",
    "{chat_history}\n",
    "\n",
    "#Question: \n",
    "{question} \n",
    "\n",
    "#Context: \n",
    "{context} \n",
    "\n",
    "#Answer:\\n\\n\n",
    "{format_instructions}\"\"\"\n",
    ")\n",
    "\n",
    "prompt = prompt.partial(format_instructions=json_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 언어 모델\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retrieval,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"chat_history\": itemgetter(\"chat_history\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    # | StrOutputParser()\n",
    "    | json_parser\n",
    ")\n",
    "\n",
    "\n",
    "# 세션 기록을 저장할 딕셔너리\n",
    "store = {}\n",
    "\n",
    "\n",
    "# 세션 ID를 기반으로 세션 기록을 가져오는 함수\n",
    "def get_session_history(session_id):\n",
    "    print(f\"Session ID: {session_id}\")\n",
    "    if session_id not in store:  # 세션 ID가 store에 없는 경우\n",
    "        # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]  # 해당 세션 ID에 대한 세션 기록 반환\n",
    "\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,  # 세션 기록을 가져오는 함수\n",
    "    input_messages_key=\"question\",  # 사용자의 질문이 템플릿 변수에 들어갈 key\n",
    "    history_messages_key=\"chat_history\",  # 기록 메시지의 키\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session ID: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: KeyError('output')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': '파이썬 스크립트를 업로드하는 구체적인 방법은 제공된 문서에 명시되어 있지 않습니다. 다만, 스크립트 작성과 관련된 내용으로, zip 파일 압축 해제 후 스크립트를 실행하는 방법, 파일 쓰기 스크립트 작성, 변수를 사용한 스크립트 작성, 그리고 예외 처리 방법 등이 언급되어 있습니다. \\n\\n스크립트 업로드 방법을 알기 위해서는 해당 시스템이나 플랫폼에 대한 추가 정보가 필요합니다. 어떤 시스템에 스크립트를 업로드하려는지 알려주시면 더 정확한 답변을 드릴 수 있을 것 같습니다.', 'is_context_relevant': True}\n"
     ]
    }
   ],
   "source": [
    "question = \"파이썬 스크립트 업로드 하는 방법 알려줘\"\n",
    "uuid = \"test\"\n",
    "\n",
    "answer = chain_with_history.invoke({\"question\": question},\n",
    "                                    config={\"session_id\": uuid})\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session ID: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: KeyError('output')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': '안녕하세요! 무엇을 도와드릴까요?', 'is_context_relevant': False}\n"
     ]
    }
   ],
   "source": [
    "question = \"안녕\"\n",
    "uuid = \"test\"\n",
    "\n",
    "answer = chain_with_history.invoke({\"question\": question},\n",
    "                                    config={\"session_id\": uuid})\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer['is_context_relevant']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
